{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14205152-5d7c-4493-b46f-a6b471f4816d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import pubsub_v1\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c894e0-d51a-44b8-8640-fe8350ca4712",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/pkk2125/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407c2868a0ce430b997f49051cfafc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA loaded from  ./weights/lora_llm.ckpt\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from hf_token import hf_token\n",
    "from utils.lora_ckpt import load_lora\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "access_token = hf_token\n",
    "login(token=access_token)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=access_token)\n",
    "\n",
    "lora_modules = ['q_proj', 'v_proj']\n",
    "lora_r = 16\n",
    "lora_alpha = lora_r\n",
    "lora_dropout = 0.05\n",
    "\n",
    "weights_path = './weights/lora_llm.ckpt'\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "        r = lora_r,\n",
    "        lora_alpha = lora_alpha,\n",
    "        target_modules = lora_modules,\n",
    "        lora_dropout = lora_dropout,\n",
    "        bias = \"none\"\n",
    ")\n",
    "lora_llm = get_peft_model(model=model, peft_config=lora_config)\n",
    "load_lora(lora_llm, weights_path, end_of_epoch=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d15231-d41a-48b9-8006-905fbfd336f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_prompt(llm, tokenizer, prompt, device='cuda'):\n",
    "    # Tokenize\n",
    "    tokens = tokenizer(\n",
    "        prompt, padding=True, return_tensors='pt'\n",
    "    )['input_ids'].to(device)\n",
    "    \n",
    "    # Encode\n",
    "    words_embed = llm(\n",
    "        tokens, output_hidden_states=True\n",
    "    ).hidden_states[-1] # last layer\n",
    "\n",
    "    return words_embed[:, -1, :] # last or EOS token\n",
    "\n",
    "tokenizer.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cb3d149-efbe-497e-a7b7-f191df01caab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import pubsub_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef259a4b-369c-4218-a007-c9c6fe792945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<StreamingPullFuture at 0x7fa7cc264f10 state=pending>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Publisher config\n",
    "\n",
    "project_id = \"eecse6992-yolov4-tiny-pkk2125\"\n",
    "\n",
    "subscription_name = \"LCCE-inference-sub\"\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "subscription_path = subscriber.subscription_path(project_id, subscription_name)\n",
    "\n",
    "prompt_buffer = None # save input prompt into this buffer\n",
    "def sub_callback(message):\n",
    "    # prompt_buffer.append(message.data)\n",
    "    prompt_buffer = message.data\n",
    "    message.ack()\n",
    "    \n",
    "subscriber.subscribe(subscription_path, callback=sub_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c2c08de-89bd-4df7-9114-1509b39d803a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_test = \"Yes this is a test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a988031-d592-412c-843b-95de86d081ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(prompt, llm=model, tokenizer=tokenizer, device='cpu'): # get embedding from self queue\n",
    "        embedding = read_prompt(llm, tokenizer, prompt, device)\n",
    "        return embedding\n",
    "if(prompt_buffer is not None):    \n",
    "    prompt_embedding = get_embedding(prompt_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2f4d035-1e07-453c-8fc4-b495fa070d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tobytes(prompt_embedding):\n",
    "    return prompt_embedding.detach().numpy().tobytes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df817c5a-fa97-45ca-a169-7c0df5076d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_embedding = prompt_embedding.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1ee1847-dee2-4e83-a891-9627bde6f81d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63cbca23-c53b-4e02-822d-08adc125b3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a038c9c-df5b-4396-9b4b-c669d7f1a0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending b'testing_prompt_0'\n",
      "Sending b'testing_prompt_1'\n",
      "Sending b'testing_prompt_2'\n",
      "Sending b'testing_prompt_3'\n",
      "Sending b'testing_prompt_4'\n",
      "Sending b'testing_prompt_5'\n",
      "Sending b'testing_prompt_6'\n",
      "Sending b'testing_prompt_7'\n",
      "Sending b'testing_prompt_8'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     prompt_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m     prompt_data \u001b[38;5;241m=\u001b[39m prompt_data\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prompt_data = tobytes(prompt_embedding)\n",
    "prompt_data = \"testing_prompt\"\n",
    "i=0\n",
    "while(True):\n",
    "    time.sleep(5)\n",
    "    prompt_data += f'_{i}'\n",
    "    prompt_data = prompt_data.encode('utf-8')\n",
    "\n",
    "    print(f'Sending {prompt_data}')\n",
    "    pub_topic_name = \"LCEE_prompt_publish\"\n",
    "    publisher = pubsub_v1.PublisherClient()\n",
    "    pub_topic_path = publisher.topic_path(project_id, pub_topic_name)\n",
    "    future = publisher.publish(pub_topic_path, prompt_data)\n",
    "    future.result()\n",
    "    prompt_data = \"testing_prompt\"\n",
    "    i+=1\n",
    "    if(i==10):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
