seed: 1234
__set_seed: !apply:torch.manual_seed [!ref <seed>]
np_rng: !new:numpy.random.RandomState [!ref <seed>]

save_folder: !PLACEHOLDER

# Testing on different cases

case: 2Speech2Audio
manifest_root: !apply:hparams.configs.get_manifest_path
    case: !ref <case>
TaskHandler: !apply:hparams.configs.get_task_handler
    case: !ref <case>
special_prompts: !apply:hparams.configs.get_special_prompts
    case: !ref <case>

# Data

test_pattern: !apply:os.path.join [!ref <manifest_root>, 'test_*k.json']
test_files: !apply:glob.glob [!ref <test_pattern>]

n_test: 0

prompt_builder: !new:data.datasets.prompt_templates.ShortTemplate
    acts: ['0', '1', 'D' ,'U']
    shuffle: true
    random: true
    
prob_gpt_prompt: 1.0
rand_prompt: true
rand_tasks: false

ret_src: false

test_set: !new:data.datasets.prompt_mixtures.PromptMixtures
    manifest_files: !ref <test_files>
    select_n: !ref <n_test>
    task_handler: !ref <TaskHandler>
    rand_tasks: false # depreciated
    prob_gpt_prompt: 1.0
    rand_prompt: false
    prompt_builder: null
    delta_styles: true
    special_prompts: !ref <special_prompts>
    prob_special_prompt: 0.5
    ret_src: !ref <ret_src>

# Loader

test_loader_opts:
    batch_size: 1
    num_workers: 1
    shuffle: false

# Speedup

mix_prec: false
mix_dtype: !name:torch.bfloat16 # always
llm_mix_prec: false

# Text encoder

llm_path: /engram/naplab/shared/LLaMA2/huggingface/Llama-2-7b-chat-hf

tokenizer: !apply:transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: !ref <llm_path>
    add_eos_token: !ref <add_eos>

llm: !apply:transformers.AutoModelForCausalLM.from_pretrained
    pretrained_model_name_or_path: !ref <llm_path>

add_eos: true 

# LoRA

lora_modules: ['q_proj', 'v_proj']
lora_r: 16
lora_alpha: !ref <lora_r>
lora_dropout: 0.05

lora_config: !new:peft.LoraConfig
    r: !ref <lora_r>
    lora_alpha: !ref <lora_alpha>
    target_modules: !ref <lora_modules>
    lora_dropout: !ref <lora_dropout>
    bias: "none"

lora_llm: !apply:peft.get_peft_model
    model: !ref <llm>
    peft_config: !ref <lora_config>

# Everything

modules:
    lora_llm: !ref <lora_llm>

# Log and save

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    custom_load_hooks:
        lora_llm: !name:utils.lora_ckpt.load_lora
    custom_save_hooks:
        lora_llm: !name:utils.lora_ckpt.save_lora
    recoverables:
        lora_llm: !ref <lora_llm>