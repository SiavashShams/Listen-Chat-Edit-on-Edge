{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d7912-dd97-446d-b044-c3ccc54f1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f198df-8911-437e-b671-9971e9665579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/pkk2125/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96caf7742ff543a4b1d4aa5f88656574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from hf_token import hf_token\n",
    "\n",
    "access_token = hf_token\n",
    "login(token=access_token)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0dbaf8-5e2b-47ca-9754-7ed414cd3f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.lora_ckpt import load_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "277de9b7-c194-456a-9d95-7e8a128a485d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "632720cb-922a-4207-b4c7-56192ed90e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_modules = ['q_proj', 'v_proj']\n",
    "lora_r = 16\n",
    "lora_alpha = lora_r\n",
    "lora_dropout = 0.05\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "        r = lora_r,\n",
    "        lora_alpha = lora_alpha,\n",
    "        target_modules = lora_modules,\n",
    "        lora_dropout = lora_dropout,\n",
    "        bias = \"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f2ae48c-51fd-4197-87a2-73c266fb44dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_llm = get_peft_model(model=model, peft_config=lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "961ee183-9c89-4963-9ed4-75dc67c7d696",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA loaded from  ./weights/lora_llm.ckpt\n"
     ]
    }
   ],
   "source": [
    "load_lora(lora_llm, weights_path, end_of_epoch=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e39ff9-29e3-4429-93e4-2b01719d1068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_prompt(llm, tokenizer, prompt, device='cuda'):\n",
    "    # Tokenize\n",
    "    tokens = tokenizer(\n",
    "        prompt, padding=True, return_tensors='pt'\n",
    "    )['input_ids'].to(device)\n",
    "    \n",
    "    # Encode\n",
    "    words_embed = llm(\n",
    "        tokens, output_hidden_states=True\n",
    "    ).hidden_states[-1] # last layer\n",
    "\n",
    "    return words_embed[:, -1, :] # last or EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92a7aae8-323f-40b1-9a03-beb4d850fc97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "828634bb-9624-4e18-bd77-e32f0e20f964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = 'Remove male speaker'\n",
    "embedding = read_prompt(lora_llm, tokenizer, prompt, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53c45fba-e527-4866-958a-fdfe754f5a94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(embedding.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
